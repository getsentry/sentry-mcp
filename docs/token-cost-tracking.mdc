---
description: How to measure and track the token cost of MCP tool definitions
globs:
alwaysApply: false
---
# Token Cost Tracking

Measures the static overhead of MCP tool definitions - the tokens sent to LLM clients with every request.

## What's Being Measured

The token cost of tool metadata that MCP sends to clients via `tools/list`:
- Tool names and descriptions
- Parameter schemas (JSON Schema)
- Total overhead per tool and across all tools

**Not measured:** Runtime token usage by embedded agents (search_events, search_issues, use_sentry).

## Running Locally

**Display table (default):**
```bash
pnpm run measure-tokens
```

**Output:**
```
📊 MCP Server Token Cost Report
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Total Tokens:     8,769
Tool Count:       19
Average/Tool:     462
Estimated Cost:   $26.3070 per 1,000 requests
                  (Claude 3.5 Sonnet @ $3/1M input tokens)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Per-Tool Breakdown:

┌─────────────────────────────┬────────┬─────────┐
│ Tool                        │ Tokens │ % Total │
├─────────────────────────────┼────────┼─────────┤
│ search_docs                 │   1023 │   11.7% │
│ update_issue                │    731 │    8.3% │
...
```

**Write JSON to file:**
```bash
# From repository root
pnpm run measure-tokens -- -o token-stats.json

# Or from mcp-server package
cd packages/mcp-server
pnpm run measure-tokens -- -o token-stats.json
```

JSON format:
```json
{
  "total_tokens": 8769,
  "tool_count": 19,
  "avg_tokens_per_tool": 462,
  "estimated_cost_per_1k_requests": 26.307,
  "tools": [
    {"name": "search_docs", "tokens": 1023, "percentage": 11.7},
    ...
  ]
}
```

## CI/CD Integration

GitHub Actions workflow runs on every PR and push to main:

**On Pull Requests:**
- 📝 **PR Comment:** Automatic comment with full report (updated on each push)
- 📊 **Job Summary:** Detailed per-tool breakdown in Actions tab
- 📦 **Artifact:** `token-stats-{sha}.json` stored for 90 days

**On Main Branch:**
- 📊 **Job Summary:** Detailed per-tool breakdown in Actions tab
- 📦 **Artifact:** `token-stats-{sha}.json` stored for 90 days

**Workflow:** `.github/workflows/token-cost.yml`

## Understanding the Results

**Current baseline (19 tools):**
- ~8,700 tokens total
- ~460 tokens/tool average
- ~$26/1,000 requests for Claude 3.5 Sonnet

**Tool count limits:**
- **Target:** ≤20 tools (current best practice)
- **Maximum:** ≤25 tools (hard limit for AI agents)

**When to investigate:**
- Total tokens increase >10% without new tools
- Individual tool >1,000 tokens (indicates overly verbose descriptions)
- New tool adds >500 tokens (review description clarity)

## Implementation Details

**Tokenizer:** Uses `tiktoken` with GPT-4's `cl100k_base` encoding (good approximation for Claude).

**Cost calculation:**
```typescript
// Based on Claude 3.5 Sonnet pricing: $3.00 per 1M input tokens
cost_per_1k_requests = (total_tokens * 1000 * 3.00) / 1_000_000
```

**Script location:** `packages/mcp-server/scripts/measure-token-cost.ts`

**CLI options:**
```bash
tsx measure-token-cost.ts              # Display table
tsx measure-token-cost.ts -o file.json # Write JSON to file
tsx measure-token-cost.ts --help       # Show help
```

## Optimizing Token Cost

**Reduce description verbosity:**
- Be concise - LLMs don't need hand-holding
- Remove redundant examples
- Focus on unique, non-obvious details

**Simplify parameter schemas:**
- Use `.describe()` sparingly
- Avoid duplicate descriptions in nested schemas
- Combine related parameters

**Consolidate tools:**
- Before adding a new tool, check if existing tools can handle it
- Consider parameter variants instead of separate tools

## References

- Script: `packages/mcp-server/scripts/measure-token-cost.ts`
- Workflow: `.github/workflows/token-cost.yml`
- Tool limits: See "Tool Count Limits" in `docs/adding-tools.mdc`
